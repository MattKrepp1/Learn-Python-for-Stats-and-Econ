{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 9: Partial Correlations and Directed Acyclic Graphs\n",
    "\n",
    "The OLS regression is a powerful tool. So powerful, in fact, that you would have trouble mastering the tool in the period of just a few years. This is not to say that you could not master a particular application with OLS, however, there are many applications.\n",
    "\n",
    "Statisticians have, historically, not been fond of making causal claims using statistics aside from the causal inferences generated from randomized control trials. In recent decades, however, the statisticians behind the causal inference revolution have move precisely in this direction. In this lesson, we will consider a particular manifestation of the causal inference revolution: Directed Acyclic Graphs (DAGs). DAGs provide a causal map, creating a causal skeleton detected using computational methods.\n",
    "\n",
    "In order to understand DAGs, we must first take review the concept of partial correlation. A partial correlation is a correlation that controls for changes in a set of control variables. Fortunately, we can calculate partial correlations using a set of ordinary least squares regression. Suppose that we want to calculate the partial correlation of variables $Q$ and $X$ controlling for $Y$ and $Z$. This would be accomplished by running four regressions, one for each variable. Across the set of regressions, each variable of interest is treated as the endogenous variable with the remaining variables be treated as exogensous variables. We can envision a vector of y vectors:\n",
    "\n",
    "$y = [y_j, y_{j+1}, . . . , y_{n-1}, y_n]$\n",
    "\n",
    "For every $y_j$, we will run a regression where the remaining vectors comprise the matrix X, which includes a vector of 1s: \n",
    "\n",
    "$y_j = \\beta_0 +\\sum^{m}_{j=1}\\beta_j x_{i,j} + \\epsilon_j$\n",
    "\n",
    "To aid our understanding of this process, we will create 5 variables using a random number generator. We will first calculate partial correlations among these variables. Then we will generate a undirected graph that will form the basis of the directed acyclic graph that we ultimately aim to build. Since we created the variables, we know which variable *causes* another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.768015</td>\n",
       "      <td>52.301438</td>\n",
       "      <td>162.268138</td>\n",
       "      <td>-57.831180</td>\n",
       "      <td>396.347465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.914048</td>\n",
       "      <td>45.954428</td>\n",
       "      <td>145.277606</td>\n",
       "      <td>-44.794589</td>\n",
       "      <td>346.101020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.296998</td>\n",
       "      <td>45.172715</td>\n",
       "      <td>139.682249</td>\n",
       "      <td>-50.603239</td>\n",
       "      <td>354.371233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.034140</td>\n",
       "      <td>47.325112</td>\n",
       "      <td>145.683400</td>\n",
       "      <td>-46.817730</td>\n",
       "      <td>360.270161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.728518</td>\n",
       "      <td>56.394225</td>\n",
       "      <td>169.860023</td>\n",
       "      <td>-61.175182</td>\n",
       "      <td>428.583268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>23.203684</td>\n",
       "      <td>54.131565</td>\n",
       "      <td>161.470056</td>\n",
       "      <td>-70.450943</td>\n",
       "      <td>432.369748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>28.178935</td>\n",
       "      <td>43.709582</td>\n",
       "      <td>133.678347</td>\n",
       "      <td>-35.107911</td>\n",
       "      <td>325.976549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>31.681515</td>\n",
       "      <td>51.712940</td>\n",
       "      <td>143.471577</td>\n",
       "      <td>-49.619787</td>\n",
       "      <td>378.709698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>25.525426</td>\n",
       "      <td>61.615787</td>\n",
       "      <td>184.669660</td>\n",
       "      <td>-70.921518</td>\n",
       "      <td>476.549733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>26.631961</td>\n",
       "      <td>39.657692</td>\n",
       "      <td>119.187362</td>\n",
       "      <td>-41.160316</td>\n",
       "      <td>313.301508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               P          Q           X          Y           Z\n",
       "0      20.768015  52.301438  162.268138 -57.831180  396.347465\n",
       "1      19.914048  45.954428  145.277606 -44.794589  346.101020\n",
       "2      23.296998  45.172715  139.682249 -50.603239  354.371233\n",
       "3      24.034140  47.325112  145.683400 -46.817730  360.270161\n",
       "4      27.728518  56.394225  169.860023 -61.175182  428.583268\n",
       "...          ...        ...         ...        ...         ...\n",
       "99995  23.203684  54.131565  161.470056 -70.450943  432.369748\n",
       "99996  28.178935  43.709582  133.678347 -35.107911  325.976549\n",
       "99997  31.681515  51.712940  143.471577 -49.619787  378.709698\n",
       "99998  25.525426  61.615787  184.669660 -70.921518  476.549733\n",
       "99999  26.631961  39.657692  119.187362 -41.160316  313.301508\n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "length = 100000\n",
    "cols = [\"P\", \"Q\", \"X\", \"Y\", \"Z\"]\n",
    "mu = 0\n",
    "sigma = 5\n",
    "\n",
    "\n",
    "lst_dct = {col:[] for col in cols}\n",
    "for i in range(length):\n",
    "    lst_dct[\"P\"].append(25 + np.random.normal(mu, .5 * sigma))\n",
    "    lst_dct[\"Q\"].append(50 + np.random.normal(mu, sigma))\n",
    "    lst_dct[\"X\"].append(3 * lst_dct[\"Q\"][-1]  + np.random.normal(mu, sigma ))\n",
    "    lst_dct[\"Y\"].append(lst_dct[\"Q\"][-1] * -1 +  np.random.normal(mu, sigma))\n",
    "    lst_dct[\"Z\"].append(\n",
    "        lst_dct[\"P\"][-1] * 2 +  1.5 * lst_dct[\"X\"][-1] -  lst_dct[\"Y\"][-1] * 2 +  np.random.normal(mu,  sigma))\n",
    "\n",
    "df = pd.DataFrame(lst_dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these variables are correlated. Variables X, Y, and Z all descend from Q. X and Y are direct descendants of Q whereas Z is a direct descendant of X, Y, and P. We can view some of these correlation in a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Q', ylabel='Z'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# We can visualize the correlation of these variables on four dimensions:\n",
    "#     2 dimensions in x-y space, a 3rd dimension using size and a fourth using color\n",
    "plt.rcParams.update({\"font.size\":26})\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "df.plot.scatter(x = \"Q\", y = \"Z\", c = \"Y\",s = \"X\", cmap = \"viridis_r\",alpha = .8, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Correlation\n",
    "\n",
    "All of the variables appear to be correlated. Yet, we should be suspicious of the correlation between $Z$ and $Q$. $Q$ does not cause $Z$ directly. Its causal effect on Z is intermediated by $X$ and $Y$. Likewise, $X$ and $Y$ appear to be strongly correlated, but we know that these variables do not cause one another. They have a common cause.\n",
    "\n",
    "To calculate correlation between two variables while controlling for the effects of the remaining variables, we calculate the partial correlation. For example, the partial correlation of $Q$ and $X$ is an estimate of the correlation of the two variables controlling for $Y$ and $Z$: $\\rho_{QX.YZ}$. \n",
    "\n",
    "It is convenient that a partial correlation can be calculated using a set of OLS regressions equal in number to the number of variables, with each serving as the endogenous variable to be estimated by the remaing variables and a constant. We calculate the partial correlation of two variables by calculating the correlation of the error terms from the regressions that estimate $Q$ and $X$ as dependent variables, given the example in the previous paragraph. Thus, in our example, we can calculate partial correlations with up to 5 different OLS equations. If all 5 equations are used, then the partial correlation will control for the remaining 3 variables. As we will see, there may be reason to exclude one or more controls variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "residuals = {}\n",
    "for y_var in df.keys():\n",
    "    X_vars = list(df.keys())\n",
    "    X_vars.remove(y_var)\n",
    "    X = df[X_vars]\n",
    "    # Initial estimate should include constant\n",
    "    #   This won't be the case we regress the errors\n",
    "    X[\"Constant\"] = 1\n",
    "    # pass y_var as list for consistent structure\n",
    "    y = df[[y_var]]\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    residuals[y_var] = results.resid\n",
    "residuals = pd.DataFrame(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial correlation is the negative correlation of the residuals generated by the above set of regressions. For all relevant partial correlations ($|pcorr(A,B)| < 1$), we multiply the partial correlation by $-1$. If you like, you can use df.fillna(1) to replace the null values in the diagonal of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.corr()[residuals.corr().abs() < 1].mul(-1).fillna(1).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pingouin\n",
    "import pingouin\n",
    "df.pcorr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datlib.plots import *\n",
    "corr_matrix_heatmap(df.corr(), \n",
    "                    save_fig = False, \n",
    "                    pp = None, \n",
    "                    title = \"Correlation\")\n",
    "corr_matrix_heatmap(df.pcorr(), save_fig = False, pp = None, title = \"Partial Correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is useful, we will need to calculate the statistical significance of the these partial correlations. To help develop an intuitive interpretation, we will regress one error term on the other with no constant included on the right hand side of the regression equation. The p-value of the beta estimate is the p-value of the partial correlation. Later on, we will use the *pingouin* module to perform these calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcorr_pvalues = {}\n",
    "for y, Y in residuals.items():\n",
    "    pcorr_pvalues[y] = {}\n",
    "    for x, X in residuals.items():\n",
    "        if x != y:\n",
    "            pcorr_pvalues[y][x] = sm.OLS(Y,X).fit().pvalues[x]\n",
    "        \n",
    "        else:\n",
    "            pcorr_pvalues[y][x] = np.NaN\n",
    "pd.DataFrame(pcorr_pvalues).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that not every partial correlation in the matrix is significant. In particular, we can be certain that links $PQ$ and $QZ$ will not be present in our graphs.\n",
    "\n",
    "## Using partial correlations to build a causal skeleton\n",
    "Now that we understand how to generate partial correlations, we can use this concept to generate a causal skeleton. We follow a simple algorithm in this step.\n",
    "\n",
    "1. Build a fully connected graph\n",
    "2. For each variable, test every possible with the remaining variables. If a correlation between, for example, variables $X$ and $Y$ is not statistically significant, remove the link $XY$.\n",
    "3. For variables whose correlation is statistically significant, test every possible combination of control variables. If the statistical significance of any of the tested partial correlations is broken, remove the link $XY$.\n",
    "4. If after steps 2 and 3, the link $XY$ remains that link will be included in the skeleton.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "undirected_graph = {key:[] for key in df.keys()}\n",
    "for x in undirected_graph:\n",
    "    remaining_vars = [y for y in df.keys() if y != x]\n",
    "    for y in remaining_vars:\n",
    "        undirected_graph[x].append(y)\n",
    "\n",
    "undirected_graph           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "p_val = .01\n",
    "def build_skeleton(df, undirected_graph):    \n",
    "    def check_remaining_controls(control_vars, undirected_graph, x, y, controls_used) :\n",
    "        for c_var in control_vars:\n",
    "            # set c_used every time use cycle through a new control\n",
    "            #  the program will then iterate through remaining controls\n",
    "            #  until statistical significance is broken\n",
    "            c_used = copy.copy(controls_used)\n",
    "            if y in undirected_graph[x]:\n",
    "\n",
    "                c_used.append(c_var)\n",
    "                test = df.partial_corr(x = x, y = y, covar=c_used,\n",
    "                                      method = \"pearson\")\n",
    "                if test[\"p-val\"].values[0] > p_val: \n",
    "\n",
    "                    undirected_graph[x].remove(y)\n",
    "                    #breakout of the for \n",
    "                    break\n",
    "                else:\n",
    "                    remaining_controls = copy.copy(control_vars)\n",
    "                    remaining_controls.remove(c_var)\n",
    "                    # recursive function that iterates through remaining variables \n",
    "                    #  uses them as controls statistical significance holds without them,\n",
    "                    #  otherwise break\n",
    "                    check_remaining_controls(remaining_controls, undirected_graph, x, y, c_used)\n",
    "                \n",
    "    for x in df.keys():\n",
    "        ys = undirected_graph[x]\n",
    "        for y in df.keys():\n",
    "            if x != y:\n",
    "            # first check for correlation with no controls\n",
    "                test = df.partial_corr(x = x, \n",
    "                                       y = y, \n",
    "                                       covar = None,\n",
    "                                       method = \"pearson\") \n",
    "                if test[\"p-val\"].values[0] > p_val:\n",
    "                    undirected_graph[x].remove(y)\n",
    "            # if correlated check for deseparation controlling for other variables\n",
    "                else:\n",
    "                    control_vars = [z for z in df.keys() if z != y and z != x]\n",
    "                    check_remaining_controls(control_vars, undirected_graph, x, y, [])\n",
    "    return undirected_graph\n",
    "\n",
    "undirected_graph = build_skeleton(df, undirected_graph)                                   \n",
    "undirected_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing the skeleton\n",
    "\n",
    "Next we will graph the skeleton. Since we do not know the direction of causality, we are unable to estimate $p(Y|do(X))$ - which is the estimate of each $X$'s influence on a given $Y$ controlling for confounding variables - for each pair of variables. With this mind, we will use the partial correlations calculated simultaneous to label links connecting variables. Since this includes all controls, we will need to reestimate partial correlations using information for directed edges from the *estimate()* method provided in the *pgmpy* module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "def graph_DAG(undirected_graph, df, title = \"DAG Structure\"):\n",
    "    \n",
    "    # generate partial correlation matrix to draw values from\n",
    "    # for graph edges\n",
    "    pcorr_matrix = df.pcorr()\n",
    "    graph = nx.Graph()\n",
    "    edges = []\n",
    "    edge_labels = {}\n",
    "    for key in undirected_graph:\n",
    "        for key2 in undirected_graph[key]:\n",
    "            if (key2, key) not in edges:\n",
    "                edge = (key.replace(\" \",\"\\n\"), key2[0].replace(\" \",\"\\n\"))\n",
    "                edges.append(edge)\n",
    "                # edge label is partial correlation between\n",
    "                # key and key2\n",
    "                edge_labels[edge] = str(round(pcorr_matrix.loc[key][key2],2))\n",
    "\n",
    "    # edge format: (\"i\", \"j\") --> from node i to node j\n",
    "    graph.add_edges_from(edges)\n",
    "    color_map = [\"C0\" for g in graph]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,12))\n",
    "    graph.nodes()\n",
    "    plt.tight_layout()\n",
    "    pos = nx.spring_layout(graph)#, k = 5/(len(sig_corr.keys())**.5))\n",
    "\n",
    "    plt.title(title, fontsize = 30)\n",
    "    nx.draw_networkx(graph, pos, node_color=color_map, \n",
    "                     node_size = 1000,\n",
    "                     with_labels=True,  arrows=False,\n",
    "                     font_size = 20, alpha = 1,\n",
    "                     font_color = \"white\",\n",
    "                     ax = ax)\n",
    "    nx.draw_networkx_edge_labels(graph,pos,\n",
    "                                 edge_labels=edge_labels,\n",
    "                                 font_color='green',\n",
    "                                 font_size=20)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"g1.png\", format=\"PNG\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_DAG(undirected_graph, df, title = \"Undirected Graph with Partial Correlations\\nfrom Full Set of Controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating a Directed Acyclic Graph\n",
    "\n",
    "Now that we have learned to generate the structure of an undirected graph, we have enough familiarity with the concept of a directed acyclc graph (DAG) that we can use the __*pgmpy*__ module to construct a graph with directed edges.\\*\n",
    "\n",
    "We will use the parallel PC algorithm to orient edges. No edge will be left undirected. \n",
    "\n",
    "\\*A number of different algorithms are used to direct edges. For more information, see the script from [*pgmpy'*s PC algorithm module](https://github.com/pgmpy/pgmpy/blob/dev/pgmpy/estimators/PC.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import PC\n",
    "c = PC(df)\n",
    "max_cond_vars = len(df.keys()) - 2\n",
    "\n",
    "\n",
    "model = c.estimate(return_type = \"dag\",variant= \"parallel\",#\"orig\", \"stable\"\n",
    "                   significance_level = p_val, \n",
    "                   max_cond_vars = max_cond_vars, ci_test = \"pearsonr\")\n",
    "edges = model.edges()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import ArrowStyle\n",
    "\n",
    "def graph_DAG(edges, df, title = \"\"):\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(edges)\n",
    "    color_map = [\"C0\" for g in graph]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,12))\n",
    "    graph.nodes()\n",
    "    plt.tight_layout()\n",
    "    pos = nx.spring_layout(graph)#, k = 5/(len(sig_corr.keys())**.5))\n",
    "\n",
    "    plt.title(title, fontsize = 30)\n",
    "    nx.draw_networkx(graph, pos, node_color=color_map, node_size = 1200,\n",
    "                     with_labels=True,  arrows=True,\n",
    "                     font_color = \"white\",\n",
    "                     font_size = 26, alpha = 1,\n",
    "                     width = 1, edge_color = \"C1\",\n",
    "                     arrowstyle=ArrowStyle(\"Fancy, head_length=3, head_width=1.5, tail_width=.1\"), ax = ax)\n",
    "\n",
    "graph_DAG(edges, df, title = \"Directed Acyclic Graph\")\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-separation\n",
    "\n",
    "Finally, we should control for variables impacting the endogenous variable. For example, if we calculate the partial correlation of $X$ and $Z$ controlling for $Y$, perhaps attempting to estimate the impact of $X$ on $Z$ as defined by $P(Z|X=x, Y)$, we will succeed in removing influence from confound effects of $Q$ on our estimate. An easy way to do this is to identify which node in a pair is a sink node, and to control for all other nodes that directly influence that sink node. If there is no confounding effect from the control, their should be no statisically significan influence on the estimate of the partial correlation. But if the control is influenced by a confounder or is itself a confounder, than controlling for this variable fulfills the requirement of d-separation and the partial correlation should be accurate. Likewise, we do not want to control for variables that do no have influence on the sink-node. For example, it is sufficient to estimate the correlation of $Q$ and $X$ without controlling for any other variables. The estimate is signicantly larger than the partial correlation estimated in the undirected graph.\n",
    "\n",
    "This exemplifies a problem that commonly arises with the employment of OLS regressions. The addition of exogenous variables to explain the value of a dependent variable can lead to erroneous estimation of marginal effects. As you can see from the distinction between the partial correlations in included in the graphical skeleton as compared to the partial correlations included in the DAG, the effects of $Q$ on $X$ and $Y$ are under estimated due to the inclusion of $Z$. Neither should P be included in this estimation, though its independence from $Q$ should limit its impact on estimation partial correlations of $QX$ and $QY$. We may also note the the partial correlation of $PZ$ is essentially uneffected by inclusion of the other variables since the value of $P$ is generated independently of the remaining variables in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def graph_DAG(edges, df, title = \"\"):\n",
    "    graph = nx.DiGraph()\n",
    "    edge_labels = {}\n",
    "    ############ Add ############\n",
    "    for edge in edges:\n",
    "        controls = [key for key in df.keys() if key not in edge]\n",
    "        controls = list(set(controls))\n",
    "        keep_controls = []\n",
    "        for control in controls:\n",
    "            control_edges = [ctrl_edge for ctrl_edge in edges if control == ctrl_edge[0] ]\n",
    "            if (control, edge[1]) in control_edges:\n",
    "                print(\"keep control:\", control)\n",
    "                keep_controls.append(control)                \n",
    "        print(edge, keep_controls)\n",
    "        pcorr = df[[edge[0], edge[1]]+keep_controls].pcorr()\n",
    "#         corr_matrix_heatmap(pcorr, save_fig = False, pp = None, title = \"Partial Correlation\")\n",
    "        edge_labels[edge] = str(round(pcorr[edge[0]].loc[edge[1]],2))\n",
    "    graph.add_edges_from(edges)\n",
    "    color_map = [\"grey\" for g in graph]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,12))\n",
    "    graph.nodes()\n",
    "    plt.tight_layout()\n",
    "    pos = nx.spring_layout(graph)#, k = 5/(len(sig_corr.keys())**.5))\n",
    "\n",
    "    plt.title(title, fontsize = 30)\n",
    "    nx.draw_networkx(graph, pos, node_color=color_map, node_size = 1200,\n",
    "                     with_labels=True,  arrows=True,\n",
    "                     # turn text black for larger variable names in homework\n",
    "                     font_color = \"k\",\n",
    "                     font_size = 26, alpha = 1,\n",
    "                     width = 1, edge_color = \"C1\",\n",
    "                     arrowstyle=ArrowStyle(\"Fancy, head_length=3, head_width=1.5, tail_width=.1\"), ax = ax)\n",
    "    ############ Add ############\n",
    "    nx.draw_networkx_edge_labels(graph,pos,\n",
    "                                edge_labels=edge_labels,\n",
    "                                font_color='green',\n",
    "                                font_size=20)\n",
    "\n",
    "graph_DAG(edges, df, title = \"Directed Acyclic Graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "1. Create a graphical skeleton like the one created above using the Economic Freedom of the World Index and Real GDP. \n",
    "2. Create a directed acyclic graph using the same data. Include partial correlations as explained in the **D-Separation** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fraserDataWithRGDPPC.csv\", \n",
    "                   index_col=[0,1],\n",
    "                  parse_dates = True).dropna()\n",
    "years = np.array(sorted(list(set(data.index.get_level_values(\"Year\")))))\n",
    "years = pd.date_range(years[0], years[-2], freq = \"AS\")\n",
    "countries = sorted(list(set(data.index.get_level_values(\"ISO_Code\"))))\n",
    "index_names = list(data.index.names)\n",
    "multi_index = pd.MultiIndex.from_product([countries, \n",
    "                                          years[:-1]], \n",
    "                                         names =data.index.names)\n",
    "data = data.reindex(multi_index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these  problems, you will need to consider how to transform real GDP. You could calculate the rate of change of real GDP and leave the index values in their original form. Or after calculating the rate of change of real GDP, you could take the difference of all columns. This is what I elect to do in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(data.keys())[1:]\n",
    "df = data.copy()[keys]\n",
    "# df =df[df[\"RGDP Per Capita\"]>10000].diff()\n",
    "df[\"RGDP Per Capita\"] = data[\"RGDP Per Capita\"].groupby(\"ISO_Code\").pct_change()\n",
    "df = df.rename(columns={key:key.replace(\" \", \"\\n\") for key in df}).dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_DAG(edges, \n",
    "              df, \n",
    "              pp=False, \n",
    "              edge_labels = False, \n",
    "              sig_vals = [.05,.01,.001],\n",
    "              title = \"\"):\n",
    "    def build_edge_labels(edges, df, sig_vals):\n",
    "        edge_labels = {}\n",
    "        for edge in edges:\n",
    "            controls = [key for key in df.keys() if key not in edge]\n",
    "            controls = list(set(controls))\n",
    "            keep_controls = []\n",
    "            for control in controls:\n",
    "                control_edges = [ctrl_edge for ctrl_edge in edges if control == ctrl_edge[0] ]\n",
    "                if (control, edge[1]) in control_edges:\n",
    "                    keep_controls.append(control)                \n",
    "            pcorr = df.partial_corr(x = edge[0], y = edge[1], covar=keep_controls,\n",
    "                                  method = \"pearson\")\n",
    "            label = str(round(pcorr[\"r\"][0],2))\n",
    "            pvalue = pcorr[\"p-val\"][0]\n",
    "#             pcorr = df[[edge[0], edge[1]]+keep_controls].pcorr()\n",
    "#             label = pcorr[edge[0]].loc[edge[1]]\n",
    "\n",
    "            for sig_val in sig_vals:\n",
    "                if pvalue < sig_val: \n",
    "                    label = label + \"*\"   \n",
    "            \n",
    "            edge_labels[edge] = label\n",
    "        return edge_labels\n",
    "    graph = nx.DiGraph()\n",
    "    if edge_labels == False:\n",
    "        edge_labels = build_edge_labels(edges, \n",
    "                                        df, \n",
    "                                        sig_vals=sig_vals) \n",
    "    graph.add_edges_from(edges)\n",
    "    color_map = [\"C0\" for g in graph]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,20))\n",
    "    graph.nodes()\n",
    "    plt.tight_layout()\n",
    "    pos = nx.spring_layout(graph)#, k = 5/(len(sig_corr.keys())**.5))\n",
    "\n",
    "    nx.draw_networkx(graph, pos, node_color=color_map, node_size = 2500,\n",
    "                     with_labels=True,  arrows=True,\n",
    "                     font_color = \"k\",\n",
    "                     font_size = 26, alpha = 1,\n",
    "                     width = 1, edge_color = \"C1\",\n",
    "                     arrowstyle=ArrowStyle(\"Fancy, head_length=3, head_width=1.5, tail_width=.1\"),\n",
    "                     connectionstyle='arc3, rad = 0.05',\n",
    "                     ax = ax)\n",
    "    \n",
    "    plt.title(title, fontsize = 30)\n",
    "    edge_labels2 = []\n",
    "    for u, v, d in graph.edges(data=True):\n",
    "        if pos[u][0] > pos[v][0]:  \n",
    "            if (v,u) in edge_labels.keys():\n",
    "                edge_labels2.append(((u, v,), f'{edge_labels[u,v]}\\n\\n\\n{edge_labels[(v,u)]}'))  \n",
    "        if (v,u) not in edge_labels.keys():\n",
    "            edge_labels2.append(((u,v,), f'{edge_labels[(u,v)]}'))\n",
    "    edge_labels = dict(edge_labels2)\n",
    "\n",
    "    nx.draw_networkx_edge_labels(graph, \n",
    "                                 pos,\n",
    "                                 edge_labels=edge_labels, \n",
    "                                 font_color='C2')\n",
    "    \n",
    "    nx.draw_networkx_edge_labels(graph,pos,\n",
    "                                 edge_labels=edge_labels,\n",
    "                                 font_color='green',\n",
    "                                 font_size=20)\n",
    "    if pp == True:\n",
    "        pp.savefig(fig, bbox_inches = \"tight\")  \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "p_val=.05\n",
    "c = PC(df.dropna())\n",
    "max_cond_vars = len(df.keys()) - 2\n",
    "model = c.estimate(return_type = \"pdag\",\n",
    "                   variant= \"parallel\",#\"orig\", \"stable\"\n",
    "                   significance_level = p_val, \n",
    "                   max_cond_vars = max_cond_vars, \n",
    "                   ci_test = \"pearsonr\")\n",
    "edges = model.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_DAG(edges, df, title = \"Directed Acyclic Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()[keys]\n",
    "df =df[df[\"RGDP Per Capita\"]>15000]#.diff()\n",
    "df[\"RGDP Per Capita\"] = df[\"RGDP Per Capita\"].groupby(\"ISO_Code\").pct_change()\n",
    "# df = df[[\"RGDP Per Capita\", \"EFW\"]]\n",
    "df = df.rename(columns={key:key.replace(\" \", \"\\n\") for key in df}).dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "c = PC(df.dropna())\n",
    "max_cond_vars = len(df.keys()) - 2\n",
    "model = c.estimate(return_type = \"pdag\",\n",
    "                   variant= \"parallel\",#\"orig\", \"stable\"\n",
    "                   significance_level = p_val, \n",
    "                   max_cond_vars = max_cond_vars, \n",
    "                   ci_test = \"pearsonr\")\n",
    "edges = model.edges()\n",
    "graph_DAG(edges, df, title = \"Directed Acyclic Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()[keys]\n",
    "df =df[df[\"RGDP Per Capita\"] < 15000]#.diff()\n",
    "df[\"RGDP Per Capita\"] = df[\"RGDP Per Capita\"].groupby(\"ISO_Code\").pct_change()\n",
    "# df = df[[\"RGDP Per Capita\", \"EFW\"]]\n",
    "df = df.rename(columns={key:key.replace(\" \", \"\\n\") for key in df}).dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "df = df.groupby(\"ISO_Code\").diff().dropna()\n",
    "c = PC(df.dropna())\n",
    "max_cond_vars = len(df.keys()) - 2\n",
    "model = c.estimate(return_type = \"pdag\",\n",
    "                   variant= \"parallel\",#\"orig\", \"stable\"\n",
    "                   significance_level = p_val, \n",
    "                   max_cond_vars = max_cond_vars, \n",
    "                   ci_test = \"pearsonr\")\n",
    "edges = model.edges()\n",
    "graph_DAG(edges, df, title = \"Directed Acyclic Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
