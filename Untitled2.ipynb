{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Building an OLS Regression Model\n",
    "\n",
    "Having built statistics functions, we are now ready to build a function for regression analysis. We will start by building the an regression. We will use linear algebra to estimate parameters that minimize the sum of the squared errors. This is an ordinary least squares regression. \n",
    "\n",
    "An OLS regression with one exogenous variable takes the form. \n",
    "\n",
    "$y = \\alpha + \\beta_1x_1 + \\mu $\n",
    "\n",
    "$\\beta_0 = \\alpha + \\mu$\n",
    "\n",
    "We merge the error term, which represents bias in the data, with alpha to yield the constant, $\\beta_0$. This is necessary since OLS assumes an unbiased estimator where:\n",
    "\n",
    "$\\sum_{i=0}^{n-1} e_{i}=0$\n",
    "\n",
    "Each estimate of a point created from a particular observation takes the form.\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1x_{1,i} + e_i$\n",
    "\n",
    "This can be generalized to include k exogenous, variables:\n",
    "\n",
    "$y_i = \\beta_0 + (\\sum_{j=1}^{k} \\beta_jx_{i,j}) + e_i$\n",
    "\n",
    "Ideally, we want to form a prediction where, on average, the right-hand side of the equation  yields the correct value on the left-hand side. When we perform an OLS regression, we form a predictor that minimizes the sum of the distance between each predicted value and the observed value drawn from the data. For example, if the prediction for a particular value of y is 8, and the actual value is 10, the error of the prediction is -2 and the squared error is 4.\n",
    "\n",
    "To find the function that minimizes the sum squared errors, we will use matrix algebra, also known as linear algebra. For those unfamiliar, the next section uses the numpy library to perform matrix operations. For clarity, we will review the linear algebra functions that we will use with simple examples.\n",
    "\n",
    "## Linear Algebra for OLS\n",
    "\n",
    "We solve the following function for a vector of beta values ($\\beta$), constants whose values represent estimates of the effect of variables in the set **_X_** on the selected endogenously generate variable $y$. The matrix **_X_** also includes a vector of ones used to estimate the constant $\\beta_0$.\n",
    "\n",
    "$\\beta = (X'X)^{-1}X'Y$\n",
    "\n",
    "$Y =$ Observations for Endogenous Variable\n",
    "\n",
    "$X =$ Observations for Exogenous Variables\n",
    "\n",
    "$X' =$ $X$-transpose\n",
    "\n",
    "$(X'X)^{-1} =$ Inverse of $X'X$\n",
    "\n",
    "### Inverting a Matrix\n",
    "\n",
    "In reviewing the linear equation for estimating $\\beta$, we confront two unique operations worth understanding. Included in these are some key concepts in linear algebra, including the identity matrix $I$ and linear independence. The best way to understand these concepts is by working with some sample vectors. Consider the matrix $X$ consisting of vectors $x_0$,$x_1$,â€¦,$x_{n-1}$,$x_n$. We must check that these vectors are linearly independent. We do this by joining $X$ with an identity matrix and thus create:\n",
    "\n",
    "$A = [XI]$\n",
    "\n",
    "We transform this to show that the product of $A$ and $X^{-1}$ is equal to the product of and an identity matrix, $I$ and $X^{-1}$\n",
    "\n",
    "$AX^{-1} = [XI]X^{-1}$\n",
    "\n",
    "$AX^{-1} = [IX^{-1}]$\n",
    "\n",
    "Let us solve for $AX^{-1}$ using the following vectors for $X$. \n",
    "\n",
    "$\\begin{equation*}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "4 & 1 & 5 \\\\\n",
    "6 & 8 & 6\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "Concatenate a 3 X 3 identity matrix on the left of $X$:\n",
    "\n",
    "$\\begin{equation*}\n",
    "I = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "$\\begin{equation*}\n",
    "[XI] = \\begin{bmatrix}\n",
    "1 & 2 & 1 & 1 & 0 & 0 \\\\\n",
    "4 & 1 & 5 & 0 & 1 & 0 \\\\\n",
    "6 & 8 & 6 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "If we perform row operations on $A$ to transform $X$ in $[XI]$ into $I$, then we $I$ will be transformed into $X^(-1)$:\n",
    "\n",
    "$\\begin{equation*}\n",
    "[XI] = \\begin{bmatrix}\n",
    "1 & 2 & 1 & 1 & 0 & 0 \\\\\n",
    "4 & 1 & 5 & 0 & 1 & 0 \\\\\n",
    "6 & 8 & 6 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\begin{equation*}\n",
    "r_2 - 4r_1:\\begin{bmatrix}\n",
    "1 & 2 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & -7 & 1 & -4 & 1 & 0 \\\\\n",
    "6 & 8 & 6 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "$\\begin{equation*}\n",
    "r_3 - 6r_1:\\begin{bmatrix}\n",
    "1 & 2 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & -7 & 1 & -4 & 1 & 0 \\\\\n",
    "0 & -4 & 0 & -6 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
